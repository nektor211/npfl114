# Nov 21

- Bidirectional RNN [Section 10.10.3 of DLB]
- Stacked (or multi-layer) LSTM [Section 10.10.5 of DLB, *or you can find more details in [Alex Graves: Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)*]
- *Stacked LSTM with residual connections [[Yonghui Wu et al.: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)]*
- *Grid LSTM [[Nal Kalchbrenner, Ivo Danihelka, Alex Graves: Grid Long Short-Term Memory](https://arxiv.org/abs/1507.01526)]*
- Distributed representation [Sections 5.11.1, 5.11.2 and 15.4 of DLB]
- `Word2vec` word embeddings, notably the CBOW and Skip-gram architectures [[Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean: Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)]

